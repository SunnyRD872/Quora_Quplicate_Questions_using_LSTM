{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d308b7b7-4ee5-4595-a8a5-6b8447e5dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aa4682-c72b-45d6-a2b9-cb1090d21848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325261ae-1d33-4780-9748-f7c54f79a808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c42a333-bad0-42a5-96f3-bc6bddc9ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0102b83b-ba6f-447d-af3c-c1e9e1d14d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303f10c9-01e3-4de4-98e7-a2b99f4d8312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   id            200000 non-null  int64 \n",
      " 1   qid1          200000 non-null  int64 \n",
      " 2   qid2          200000 non-null  int64 \n",
      " 3   question1     200000 non-null  object\n",
      " 4   question2     199999 non-null  object\n",
      " 5   is_duplicate  200000 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0c8dd8-6be8-4faf-a661-8c746949c21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       0\n",
       "question2       1\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd15f50-e59b-4aa3-8e94-86320579cd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d65c96c-7e18-47f7-bf81-a5c5b87bfcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_duplicate\n",
      "0    125525\n",
      "1     74475\n",
      "Name: count, dtype: int64\n",
      "is_duplicate\n",
      "0    62.7625\n",
      "1    37.2375\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='is_duplicate'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df1['is_duplicate'].value_counts())\n",
    "print((df1['is_duplicate'].value_counts()/df1['is_duplicate'].count())*100)\n",
    "df1['is_duplicate'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64deb1f-6748-42d1-8199-c46c1ba65169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique questions 301630\n",
      "Number of questions getting repeated 47905\n"
     ]
    }
   ],
   "source": [
    "##repeated questions\n",
    "qid=pd.Series(df1['qid1'].tolist()+df1['qid2'].tolist())\n",
    "print('Number of unique questions',np.unique(qid).shape[0])\n",
    "x=qid.value_counts()>1\n",
    "print('Number of questions getting repeated',x[x].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10ef639c-f7ae-4088-b3d7-b69d87645429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGdCAYAAADJ6dNTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIolJREFUeJzt3XtQXOX9x/HPArIxmpALdQkJBG29FGMWy02qjonulMFM1Ng6+cNWxGk62qWN3dpK/ii0MyqZ9mcmtXNGqjbSjrZJ0xnxkhqNGEO1UW7ipVSbtKg0EUjGhg2oYHfP7w8nm2IuZmFhn8N5v2bODOfCc777uJLPnPM853hs27YFAABgiJRkFwAAAPC/CCcAAMAohBMAAGAUwgkAADAK4QQAABiFcAIAAIxCOAEAAEYhnAAAAKOkJbuAeEWjUe3fv1+zZs2Sx+NJdjkAAOAU2Latw4cPKzs7WykpJ7824rhwsn//fuXk5CS7DAAAMA69vb1atGjRSY9xXDiZNWuWpE8/3OzZs5NcDQAAOBXhcFg5OTmxf8dPxnHh5MitnNmzZxNOAABwmFMZksGAWAAAYBTHhBPLspSfn6/i4uJklwIAACaRx7ZtO9lFxCMcDisjI0ODg4Pc1gEAwCHi+ffbMVdOAACAOxBOAACAUQgnAADAKIQTAABgFMIJAAAwCuEEAAAYxTHhhOecAADgDjznBAAATDqecwIAAByLcAIAAIxCOPmMvJptyqvZluwyAABwLcIJAAAwCuEEAAAYhXACAACMQjgBAABGIZwAAACjEE4AAIBRHBNOeHw9AADu4JhwEgwG1d3drba2tmSXAgAAJpFjwgkAAHAHwgkAADAK4QQAABiFcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFEIJwAAwCiEEwAAYBTCCQAAMArhBAAAGCUtGSfNy8vT7NmzlZKSorlz52rnzp3JKAMAABgoKeFEkv7617/qzDPPTNbpAQCAobitAwAAjBJ3OGlpadHKlSuVnZ0tj8ejpqamY46xLEt5eXmaMWOGSktL1draOma/x+PRFVdcoeLiYj366KPjLh4AAEw/cYeT4eFh+f1+WZZ13P1btmxRKBRSXV2dOjs75ff7VV5eroGBgdgxL774ojo6OvTEE0/onnvu0euvvz7+TwAAAKaVuMNJRUWF7rrrLq1ateq4+zds2KA1a9aoqqpK+fn5amho0MyZM7Vp06bYMQsXLpQkLViwQFdffbU6OztPeL6RkRGFw+ExCwAAmL4SOuZkdHRUHR0dCgQCR0+QkqJAIKDdu3dL+vTKy+HDhyVJQ0NDev7553XhhReesM36+nplZGTElpycnESWDAAADJPQcHLw4EFFIhH5fL4x230+n/r6+iRJ/f39uuyyy+T3+3XJJZfopptuUnFx8QnbXLdunQYHB2NLb29vIksGAACGmfKpxOecc45ee+21Uz7e6/XK6/XKsixZlqVIJDKJ1QEAgGRL6JWTzMxMpaamqr+/f8z2/v5+ZWVlTajtYDCo7u5utbW1TagdAABgtoSGk/T0dBUWFqq5uTm2LRqNqrm5WWVlZYk8FQAAmKbivq0zNDSkvXv3xtZ7enrU1dWlefPmKTc3V6FQSJWVlSoqKlJJSYk2btyo4eFhVVVVJbRwAAAwPcUdTtrb27V8+fLYeigUkiRVVlaqsbFRq1ev1oEDB1RbW6u+vj4VFBRo+/btxwySjRdjTgAAcAePbdt2souIRzgcVkZGhgYHBzV79uyEt59Xs02S9M76FQlvGwAAt4rn32/erQMAAIzimHBiWZby8/NP+kwUAADgfI4JJ0wlBgDAHRwTTgAAgDsQTgAAgFEcE04YcwIAgDs4Jpww5gQAAHdwTDgBAADuQDgBAABGIZwAAACjOCacMCAWAAB3cEw4YUAsAADu4JhwAgAA3IFwAgAAjEI4AQAARiGcAAAAozgmnDBbBwAAd3BMOGG2DgAA7uCYcAIAANyBcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFEIJwAAwChpyS7gVFmWJcuyFIlEpvzceTXbYj+/s37FlJ8fAAA3ccyVE55zAgCAOzgmnAAAAHcgnAAAAKMQTgAAgFEIJwAAwCiEEwAAYBTCCQAAMArhBAAAGIVwAgAAjEI4AQAARnFMOLEsS/n5+SouLk52KQAAYBI5Jpzw+HoAANzBMeEEAAC4A+EEAAAYhXACAACMQjgBAABGIZwAAACjEE4AAIBRCCcAAMAohBMAAGAUwgkAADAK4QQAABiFcAIAAIyStHDy4YcfavHixbrjjjuSVQIAADBQ0sLJ3XffrUsuuSRZpwcAAIZKSjjZs2eP3nrrLVVUVCTj9AAAwGBxh5OWlhatXLlS2dnZ8ng8ampqOuYYy7KUl5enGTNmqLS0VK2trWP233HHHaqvrx930QAAYPqKO5wMDw/L7/fLsqzj7t+yZYtCoZDq6urU2dkpv9+v8vJyDQwMSJIef/xxnXfeeTrvvPMmVjkAAJiW0uL9hYqKipPejtmwYYPWrFmjqqoqSVJDQ4O2bdumTZs2qaamRi+//LI2b96srVu3amhoSJ988olmz56t2tra47Y3MjKikZGR2Ho4HI63ZAAA4CAJHXMyOjqqjo4OBQKBoydISVEgENDu3bslSfX19ert7dU777yj//u//9OaNWtOGEyOHJ+RkRFbcnJyElkyAAAwTELDycGDBxWJROTz+cZs9/l86uvrG1eb69at0+DgYGzp7e1NRKkAAMBQcd/WSaSbb775c4/xer3yer2TXwwAADBCQq+cZGZmKjU1Vf39/WO29/f3Kysra0JtW5al/Px8FRcXT6gdAABgtoSGk/T0dBUWFqq5uTm2LRqNqrm5WWVlZRNqOxgMqru7W21tbRMtM2HyarbFFgAAkBhx39YZGhrS3r17Y+s9PT3q6urSvHnzlJubq1AopMrKShUVFamkpEQbN27U8PBwbPYOAADAycQdTtrb27V8+fLYeigUkiRVVlaqsbFRq1ev1oEDB1RbW6u+vj4VFBRo+/btxwySjZdlWbIsS5FIZELtAAAAs8UdTpYtWybbtk96THV1taqrq8dd1PEEg0EFg0GFw2FlZGQktG0AAGCOpL34DwAA4HgcE06YrQMAgDs4JpyYOFsHAAAknmPCCQAAcAfCCQAAMArhBAAAGMUx4YQBsQAAuINjwgkDYgEAcAfHhBMAAOAOhBMAAGAUwgkAADCKY8IJA2IBAHAHx4QTBsQCAOAOjgknAADAHQgnAADAKIQTAABgFMIJAAAwimPCCbN1AABwB8eEE2brAADgDo4JJwAAwB3Skl3AdJZXsy328zvrVySxEgAAnIMrJwAAwCiEEwAAYBTCCQAAMIpjwglTiQEAcAfHhBOmEgMA4A6OCScAAMAdCCcAAMAohBMAAGAUwgkAADAK4QQAABiFcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFEcE054fD0AAO7gmHDC4+sBAHAHx4QTAADgDmnJLsCN8mq2xX5+Z/2KJFYCAIB5uHICAACMQjgBAABGIZwAAACjEE4AAIBRCCcAAMAohBMAAGAUwgkAADAK4QQAABiFcAIAAIwy5eHk0KFDKioqUkFBgZYsWaIHH3xwqksAAAAGm/LH18+aNUstLS2aOXOmhoeHtWTJEl1//fWaP3/+VJcCAAAMNOVXTlJTUzVz5kxJ0sjIiGzblm3bU10GAAAwVNzhpKWlRStXrlR2drY8Ho+ampqOOcayLOXl5WnGjBkqLS1Va2vrmP2HDh2S3+/XokWL9KMf/UiZmZnj/gAAAGB6iTucDA8Py+/3y7Ks4+7fsmWLQqGQ6urq1NnZKb/fr/Lycg0MDMSOmTNnjl577TX19PTo97//vfr7+8f/CaaRvJptY95YDACAG8UdTioqKnTXXXdp1apVx92/YcMGrVmzRlVVVcrPz1dDQ4NmzpypTZs2HXOsz+eT3+/XX/7ylxOeb2RkROFweMwCAACmr4SOORkdHVVHR4cCgcDRE6SkKBAIaPfu3ZKk/v5+HT58WJI0ODiolpYWnX/++Sdss76+XhkZGbElJycnkSUDAADDJDScHDx4UJFIRD6fb8x2n8+nvr4+SdK7776ryy+/XH6/X5dffrm+973v6aKLLjphm+vWrdPg4GBs6e3tTWTJAADAMFM+lbikpERdXV2nfLzX65XX6528ggAAgFESeuUkMzNTqampxwxw7e/vV1ZW1oTatixL+fn5Ki4unlA7AADAbAkNJ+np6SosLFRzc3NsWzQaVXNzs8rKyibUdjAYVHd3t9ra2iZaJgAAMFjct3WGhoa0d+/e2HpPT4+6uro0b9485ebmKhQKqbKyUkVFRSopKdHGjRs1PDysqqqqhBYOAACmp7jDSXt7u5YvXx5bD4VCkqTKyko1NjZq9erVOnDggGpra9XX16eCggJt3779mEGy8bIsS5ZlKRKJTKgdAABgtrjDybJlyz73cfPV1dWqrq4ed1HHEwwGFQwGFQ6HlZGRkdC2AQCAOab83ToAAAAnQzgBAABGmfLnnIwXY0405r0776xfkcRKAACYPI65csJUYgAA3MEx4QQAALgD4QQAABjFMeGEx9cDAOAOjgknjDkBAMAdHBNOAACAOzhmKjFOjCnGAIDphCsnAADAKI4JJwyIBQDAHRwTThgQCwCAOzgmnAAAAHcgnAAAAKMQTgAAgFEIJwAAwCiOCSfM1gEAwB0c8xC2YDCoYDCocDisjIyMZJfjKDykDQDgJI65cgIAANyBcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFEcM1vHsixZlqVIJJLsUqYNZvEAAEzkmCsnvPgPAAB3cEw4AQAA7kA4AQAARiGcAAAAoxBOAACAUQgnAADAKIQTAABgFMc85wRTg2efAACSjSsnAADAKIQTAABgFMeEE8uylJ+fr+Li4mSXAgAAJpFjwgmPrwcAwB0cE04AAIA7EE4AAIBRCCcAAMAohBMAAGAUwgkAADAKT4hF3HiKLABgMnHlBAAAGIVwAgAAjEI4AQAARiGcIGHyaraNGY8CAMB4MCAWU4aBtACAUzHlV056e3u1bNky5efna+nSpdq6detUlwAAAAw25VdO0tLStHHjRhUUFKivr0+FhYW6+uqrdcYZZ0x1KQAAwEBTHk4WLFigBQsWSJKysrKUmZmpDz74gHACAAAkjeO2TktLi1auXKns7Gx5PB41NTUdc4xlWcrLy9OMGTNUWlqq1tbW47bV0dGhSCSinJycuAsHAADTU9zhZHh4WH6/X5ZlHXf/li1bFAqFVFdXp87OTvn9fpWXl2tgYGDMcR988IFuuukmPfDAA+OrHAAATEtx39apqKhQRUXFCfdv2LBBa9asUVVVlSSpoaFB27Zt06ZNm1RTUyNJGhkZ0XXXXaeamhp99atfPen5RkZGNDIyElsPh8PxlgwAABwkobN1RkdH1dHRoUAgcPQEKSkKBALavXu3JMm2bd1888268sor9a1vfetz26yvr1dGRkZs4RYQAADTW0LDycGDBxWJROTz+cZs9/l86uvrkyS99NJL2rJli5qamlRQUKCCggK98cYbJ2xz3bp1GhwcjC29vb2JLBkAABhmymfrXHbZZYpGo6d8vNfrldfrncSKAACASRJ65SQzM1Opqanq7+8fs72/v19ZWVkTatuyLOXn56u4uHhC7QAAALMlNJykp6ersLBQzc3NsW3RaFTNzc0qKyubUNvBYFDd3d1qa2ubaJkAAMBgcd/WGRoa0t69e2PrPT096urq0rx585Sbm6tQKKTKykoVFRWppKREGzdu1PDwcGz2DvBZR965w/t2AADSOMJJe3u7li9fHlsPhUKSpMrKSjU2Nmr16tU6cOCAamtr1dfXp4KCAm3fvv2YQbLxsixLlmUpEolMqB0AAGC2uMPJsmXLZNv2SY+prq5WdXX1uIs6nmAwqGAwqHA4rIyMjIS2DQAAzDHlbyUGAAA4GcIJAAAwimPCCVOJAQBwB8eEE6YSAwDgDo4JJwAAwB2m/PH1wKk48uwTieefAIDbOObKCWNOAABwB8eEE8acAADgDo4JJwAAwB0YcwLHYlwKAExPXDkBAABGcUw4YUAsAADu4JhwwoBYAADcwTHhBAAAuAPhBAAAGIVwAgAAjMJUYrgC044BwDkcc+WE2ToAALiDY8IJs3UAAHAHbutgWuH2DQA4n2OunAAAAHcgnAAAAKMQTgAAgFEIJwAAwCiEEwAAYBTHhBOecwIAgDs4JpzwnBMAANzBMeEEAAC4A+EEAAAYhXACAACMQjgBAABG4d06wEnwrh4AmHqEE0CEEAAwCbd1AACAUQgnAADAKIQTAABgFMeEEx5fDwCAOzgmnPD4egAA3MEx4QQAALgD4QQAABiFcAIAAIxCOAEAAEYhnAAAAKMQToAJyqvZNubx9wCAiSGcAAAAoxBOAACAUQgnAADAKIQTAABglKSEk1WrVmnu3Ln6xje+kYzTAwAAgyUlnKxdu1a/+93vknFqIKmOzOxhdg8AnFhSwsmyZcs0a9asZJwaAAAYLu5w0tLSopUrVyo7O1sej0dNTU3HHGNZlvLy8jRjxgyVlpaqtbU1EbUCAAAXiDucDA8Py+/3y7Ks4+7fsmWLQqGQ6urq1NnZKb/fr/Lycg0MDEy4WAAAMP2lxfsLFRUVqqioOOH+DRs2aM2aNaqqqpIkNTQ0aNu2bdq0aZNqamriLnBkZEQjIyOx9XA4HHcbAADAORI65mR0dFQdHR0KBAJHT5CSokAgoN27d4+rzfr6emVkZMSWnJycRJULGIcBswCQ4HBy8OBBRSIR+Xy+Mdt9Pp/6+vpi64FAQDfccIP+/Oc/a9GiRScNLuvWrdPg4GBs6e3tTWTJAADAMHHf1kmE55577pSP9Xq98nq9k1gNAAAwSULDSWZmplJTU9Xf3z9me39/v7KysibUtmVZsixLkUhkQu0AU+F/b8u8s35FEisBAOdJ6G2d9PR0FRYWqrm5ObYtGo2qublZZWVlE2o7GAyqu7tbbW1tEy0TAAAYLO4rJ0NDQ9q7d29svaenR11dXZo3b55yc3MVCoVUWVmpoqIilZSUaOPGjRoeHo7N3gEAADiZuMNJe3u7li9fHlsPhUKSpMrKSjU2Nmr16tU6cOCAamtr1dfXp4KCAm3fvv2YQbLx4rYOcGLcRgIwncQdTpYtWybbtk96THV1taqrq8dd1PEEg0EFg0GFw2FlZGQktG0AAGCOpLxbBwAA4EQIJwAAwCiOCSeWZSk/P1/FxcXJLgUAAEwix4QTphIDAOAOjgknAADAHQgnAADAKI4JJ4w5AQDAHRwTThhzAgCAOzgmnAAAAHcgnAAAAKMQTgAAgFHifrdOsvDiP7hZol/sd6Q9XhIIwESOuXLCgFgAANzBMeEEAAC4A+EEAAAYhXACAACMQjgBAABGYbYOgGMkenYQAMTDMVdOmK0DAIA7OCacAAAAdyCcAAAAoxBOAACAUQgnAADAKIQTAABgFMIJAAAwCs85AaaxqX5eyYnOx3NTAMTDMVdOeM4JAADu4JhwAgAA3IFwAgAAjEI4AQAARiGcAAAAoxBOAACAUQgnAADAKIQTAABgFMIJAAAwCuEEAAAYxTHhxLIs5efnq7i4ONmlAACASeSYcMLj6wEAcAfHhBMAAOAOhBMAAGAUwgkAADAK4QQAABiFcAIAAIxCOAEAAEYhnAAAAKMQTgAAgFEIJwAAwCiEEwAAYBTCCQAAMEpSwslTTz2l888/X+eee64eeuihZJQAAAAMlTbVJ/zvf/+rUCiknTt3KiMjQ4WFhVq1apXmz58/1aUAAAADTfmVk9bWVl144YVauHChzjzzTFVUVOjZZ5+d6jIAAICh4g4nLS0tWrlypbKzs+XxeNTU1HTMMZZlKS8vTzNmzFBpaalaW1tj+/bv36+FCxfG1hcuXKh9+/aNr3oAADDtxB1OhoeH5ff7ZVnWcfdv2bJFoVBIdXV16uzslN/vV3l5uQYGBsZV4MjIiMLh8JgFAABMX3GPOamoqFBFRcUJ92/YsEFr1qxRVVWVJKmhoUHbtm3Tpk2bVFNTo+zs7DFXSvbt26eSkpITtldfX6+f/exn8ZYJYBLk1WyL/fzO+hWT3taJjjmy/XjbElHbZHJKnck21d81mNVPCR1zMjo6qo6ODgUCgaMnSElRIBDQ7t27JUklJSV68803tW/fPg0NDenpp59WeXn5Cdtct26dBgcHY0tvb28iSwYAAIZJ6GydgwcPKhKJyOfzjdnu8/n01ltvfXrCtDTde++9Wr58uaLRqH784x+fdKaO1+uV1+tNZJkAAMBgUz6VWJKuueYaXXPNNXH9jmVZsixLkUhkkqoCAAAmSOhtnczMTKWmpqq/v3/M9v7+fmVlZU2o7WAwqO7ubrW1tU2oHQAAYLaEhpP09HQVFhaqubk5ti0ajaq5uVllZWWJPBUAAJim4r6tMzQ0pL1798bWe3p61NXVpXnz5ik3N1ehUEiVlZUqKipSSUmJNm7cqOHh4djsHQAAgJOJO5y0t7dr+fLlsfVQKCRJqqysVGNjo1avXq0DBw6otrZWfX19Kigo0Pbt248ZJBsvxpwAAOAOcYeTZcuWybbtkx5TXV2t6urqcRd1PMFgUMFgUOFwWBkZGQltGwAAmCMpbyUGAAA4EceEE8uylJ+fr+Li4mSXAgAAJpFjwglTiQEAcAfHhBMAAOAOhBMAAGAUx4QTxpwAAOAOjgknjDkBAMAdkvLiv4k48oyVcDg8Ke1HRz48pv0j2051+3h/73jnnkh74/m9z2trMuuP5zPFW3+i/1sd73eT8Z0Z7+9Nx+/aqbRnAqfUmWyJ7Cf6/NRMdj8dafPznpUmSR77VI4yyL///W/l5OQkuwwAADAOvb29WrRo0UmPcVw4iUaj2r9/v2bNmiWPxxP374fDYeXk5Ki3t1ezZ8+ehAqdg744ir4Yi/44ir44ir4Yi/446lT6wrZtHT58WNnZ2UpJOfmoEsfd1klJSfncxHUqZs+e7fov0xH0xVH0xVj0x1H0xVH0xVj0x1Gf1xen+voZxwyIBQAA7kA4AQAARnFdOPF6vaqrq5PX6012KUlHXxxFX4xFfxxFXxxFX4xFfxyV6L5w3IBYAAAwvbnuygkAADAb4QQAABiFcAIAAIxCOAEAAEZxVTixLEt5eXmaMWOGSktL1dramuySpkRLS4tWrlyp7OxseTweNTU1jdlv27Zqa2u1YMECnX766QoEAtqzZ09yip1k9fX1Ki4u1qxZs3TWWWfpuuuu09tvvz3mmI8//ljBYFDz58/XmWeeqa9//evq7+9PUsWT5/7779fSpUtjD00qKyvT008/Hdvvln44nvXr18vj8ej222+PbXNTf/z0pz+Vx+MZs1xwwQWx/W7qC0nat2+fvvnNb2r+/Pk6/fTTddFFF6m9vT223y1/Q/Py8o75Xng8HgWDQUmJ/V64Jpxs2bJFoVBIdXV16uzslN/vV3l5uQYGBpJd2qQbHh6W3++XZVnH3f/zn/9c9913nxoaGvTKK6/ojDPOUHl5uT7++OMprnTy7dq1S8FgUC+//LJ27NihTz75RF/72tc0PDwcO+YHP/iBnnzySW3dulW7du3S/v37df311yex6smxaNEirV+/Xh0dHWpvb9eVV16pa6+9Vn/7298kuacfPqutrU2//vWvtXTp0jHb3dYfF154od5///3Y8uKLL8b2uakv/vOf/+jSSy/Vaaedpqefflrd3d269957NXfu3Ngxbvkb2tbWNuY7sWPHDknSDTfcICnB3wvbJUpKSuxgMBhbj0QidnZ2tl1fX5/EqqaeJPuxxx6LrUejUTsrK8v+xS9+Edt26NAh2+v12n/4wx+SUOHUGhgYsCXZu3btsm37089+2mmn2Vu3bo0d8/e//92WZO/evTtZZU6ZuXPn2g899JBr++Hw4cP2ueeea+/YscO+4oor7LVr19q27b7vRV1dne33+4+7z219ceedd9qXXXbZCfe7+W/o2rVr7S9+8Yt2NBpN+PfCFVdORkdH1dHRoUAgENuWkpKiQCCg3bt3J7Gy5Ovp6VFfX9+YvsnIyFBpaakr+mZwcFCSNG/ePElSR0eHPvnkkzH9ccEFFyg3N3da90ckEtHmzZs1PDyssrIy1/ZDMBjUihUrxnxuyZ3fiz179ig7O1vnnHOObrzxRr333nuS3NcXTzzxhIqKinTDDTforLPO0sUXX6wHH3wwtt+tf0NHR0f1yCOP6JZbbpHH40n498IV4eTgwYOKRCLy+Xxjtvt8PvX19SWpKjMc+fxu7JtoNKrbb79dl156qZYsWSLp0/5IT0/XnDlzxhw7XfvjjTfe0Jlnnimv16tbb71Vjz32mPLz813XD5K0efNmdXZ2qr6+/ph9buuP0tJSNTY2avv27br//vvV09Ojyy+/XIcPH3ZdX/zrX//S/fffr3PPPVfPPPOMbrvtNn3/+9/Xb3/7W0nu/Rva1NSkQ4cO6eabb5aU+P9HHPdWYiBRgsGg3nzzzTH30t3m/PPPV1dXlwYHB/WnP/1JlZWV2rVrV7LLmnK9vb1au3atduzYoRkzZiS7nKSrqKiI/bx06VKVlpZq8eLF+uMf/6jTTz89iZVNvWg0qqKiIt1zzz2SpIsvvlhvvvmmGhoaVFlZmeTqkuc3v/mNKioqlJ2dPSntu+LKSWZmplJTU48ZNdzf36+srKwkVWWGI5/fbX1TXV2tp556Sjt37tSiRYti27OysjQ6OqpDhw6NOX669kd6erq+9KUvqbCwUPX19fL7/frlL3/pun7o6OjQwMCAvvKVrygtLU1paWnatWuX7rvvPqWlpcnn87mqPz5rzpw5Ou+887R3717XfTcWLFig/Pz8Mdu+/OUvx25zufFv6LvvvqvnnntO3/72t2PbEv29cEU4SU9PV2FhoZqbm2PbotGompubVVZWlsTKku/ss89WVlbWmL4Jh8N65ZVXpmXf2Lat6upqPfbYY3r++ed19tlnj9lfWFio0047bUx/vP3223rvvfemZX98VjQa1cjIiOv64aqrrtIbb7yhrq6u2FJUVKQbb7wx9rOb+uOzhoaG9M9//lMLFixw3Xfj0ksvPeZxA//4xz+0ePFiSe77GypJDz/8sM466yytWLEiti3h34sEDtw12ubNm22v12s3Njba3d3d9ne+8x17zpw5dl9fX7JLm3SHDx+2X331VfvVV1+1JdkbNmywX331Vfvdd9+1bdu2169fb8+ZM8d+/PHH7ddff92+9tpr7bPPPtv+6KOPklx54t122212RkaG/cILL9jvv/9+bPnwww9jx9x66612bm6u/fzzz9vt7e12WVmZXVZWlsSqJ0dNTY29a9cuu6enx3799dftmpoa2+Px2M8++6xt2+7phxP539k6tu2u/vjhD39ov/DCC3ZPT4/90ksv2YFAwM7MzLQHBgZs23ZXX7S2ttppaWn23Xffbe/Zs8d+9NFH7ZkzZ9qPPPJI7Bg3/Q2NRCJ2bm6ufeeddx6zL5HfC9eEE9u27V/96ld2bm6unZ6ebpeUlNgvv/xyskuaEjt37rQlHbNUVlbatv3pVLif/OQnts/ns71er33VVVfZb7/9dnKLniTH6wdJ9sMPPxw75qOPPrK/+93v2nPnzrVnzpxpr1q1yn7//feTV/QkueWWW+zFixfb6enp9he+8AX7qquuigUT23ZPP5zIZ8OJm/pj9erV9oIFC+z09HR74cKF9urVq+29e/fG9rupL2zbtp988kl7yZIlttfrtS+44AL7gQceGLPfTX9Dn3nmGVvScT9fIr8XHtu27XFe2QEAAEg4V4w5AQAAzkE4AQAARiGcAAAAoxBOAACAUQgnAADAKIQTAABgFMIJAAAwCuEEAAAYhXACAACMQjgBAABGIZwAAACjEE4AAIBR/h8z0WBM8ybt8QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#repeated questions histogram\n",
    "plt.hist(qid.value_counts().values,bins= 160)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba29558-389b-45ea-8cc7-ee0526dbddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74bb5a58-3599-4479-8155-276976405de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    q= str(q).lower().strip()\n",
    "    # replace certain special characters with thier string\n",
    "    q= q.replace('%','percentage')\n",
    "    q= q.replace('$','dollar')\n",
    "    q= q.replace('@','at')\n",
    "    q= q.replace('?','at')\n",
    "    q= q.replace('.','at')\n",
    "    #the pattern 'math'appears almost 800 time in datasets\n",
    "    q=q.replace('[math]','')\n",
    "    #replace some  numbers eith string equivalents\n",
    "    q = q.replace(',000,000,000','b')\n",
    "    q = q.replace(',000,000','m')\n",
    "    q = q.replace(',000','k')\n",
    "    q = re.sub(r'([0-9]+)000000000',r'\\1b',q)\n",
    "    q = re.sub(r'([0-9]+)000000',r'\\1m',q)\n",
    "    q = re.sub(r'([0-9]+)000',r'\\1k',q)\n",
    "    q = re.sub(r\",\", \" \", q)\n",
    "    q = re.sub(r\"\\.\", \" \", q)\n",
    "    q = re.sub(r\"!\", \" ! \", q)\n",
    "    q = re.sub(r\"\\/\", \" \", q)\n",
    "    q = re.sub(r\"\\^\", \" ^ \", q)\n",
    "    q = re.sub(r\"\\+\", \" + \", q)\n",
    "    q = re.sub(r\"\\-\", \" - \", q)\n",
    "    q = re.sub(r\"\\=\", \" = \", q)\n",
    "    q = re.sub(r\"'\", \" \", q)\n",
    "    q = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", q)\n",
    "    q = re.sub(r\":\", \" : \", q)\n",
    "    q = re.sub(r\" e g \", \" eg \", q)\n",
    "    q = re.sub(r\" b g \", \" bg \", q)\n",
    "    q = re.sub(r\" u s \", \" american \", q)\n",
    "    q = re.sub(r\"\\0s\", \"0\", q)\n",
    "    q = re.sub(r\" 9 11 \", \"911\", q)\n",
    "    q = re.sub(r\"e - mail\", \"email\", q)\n",
    "    q = re.sub(r\"j k\", \"jk\", q)\n",
    "    q = re.sub(r\"\\s{2,}\", \" \", q)\n",
    "      #removing HTML tags\n",
    "    q= BeautifulSoup(q)\n",
    "    q= q.get_text()\n",
    "    #removing punctuation\n",
    "    q=re.sub(r'[^\\w\\s]', '', q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2691b7e-5293-48d7-a79e-64cedc5d063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['question1']= df1['question1'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01313788-688d-4bec-b492-27fe9aba82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['question2']= df1['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a55b03b9-eebb-459c-99ae-9b4a54223feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what would happen if the indian government stole the kohinoor koh  i  noor diamond backat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question2'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdfe17c8-4ae8-4039-ae03-4c8afce5954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i m a triple capricorn sun moon and ascendant in capricorn what does this say about meat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question2'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3ba1fa-3aa9-473f-a2b5-1cd5fcb87f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the story of kohinoor koh  i  noor diamondat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daeb58ef-ca3b-48e9-9788-e8c2d960198f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'astrology  i am a capricorn sun cap moon and cap risingatatatwhat does that say about meat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question1'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d41819c-4299-438f-85bc-d3263c822f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a3d738-4406-4bc7-b1ff-2101ab2744f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question1\n",
      "<class 'str'>    200000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df1['question1'].apply(type).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b186f4a3-9c5a-45d1-b9f4-c0ed3314885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def expand_contractions_in_list(data):\n",
    "    \"\"\"\n",
    "    Expands contractions in a list of strings or a single string.\n",
    "\n",
    "    Parameters:\n",
    "        data: The input, which could be a list of strings, a single string, or other types.\n",
    "\n",
    "    Returns:\n",
    "        Processed data with contractions expanded.\n",
    "    \"\"\"\n",
    "    if isinstance(data, list):  # If it's a list, process each string\n",
    "        return [contractions.fix(item) if isinstance(item, str) else item for item in data]\n",
    "    elif isinstance(data, str):  # If it's a single string, process directly\n",
    "        return contractions.fix(data)\n",
    "    return data  # Return as-is for other types (e.g., None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c3f4078-4670-4d27-8a23-e4cb5957226c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do i reset my gmail password when i don t remember my recovery informationat'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question2'][22122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c4b151d-5faf-48fe-a1dc-45e12904c015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how can you recover an gmail account without any informationat'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question1'][22122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "188b47bd-0f95-409d-a27c-8bae6b91aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['question1']= df1['question1'].apply(expand_contractions_in_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95e0f1e5-4155-4c28-a661-0fd761b314da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['question2']= df1['question2'].apply(expand_contractions_in_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f463a8ca-7ca2-41b1-9c93-7a8992b0eb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do i reset my gmail password when i don t remember my recovery informationat'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question2'][22122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c843be88-7ad6-4a24-a98b-d5a21550e08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I cannot believe it is already 5 of the clock!',\n",
       " 'You are going to love this.',\n",
       " 'It is already 5 of the clock.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"I can't believe it's already 5 o'clock!\",\n",
    "    \"You're going to love this.\",\n",
    "    \"It's already 5 o'clock.\"\n",
    "]\n",
    "expand_contractions_in_list(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d248421-7752-48d3-93fb-9817b6fdcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['q1_len']=df1['question1'].str.len()\n",
    "df1['q2_len']=df1['question2'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3360fc5a-719e-486e-b858-626dd51aee64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_new_words</th>\n",
       "      <th>q2_new_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what is the story of kohinoor koh  i  noor dia...</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>why am i mentally very lonelyat how can i solv...</td>\n",
       "      <td>find the remainder when 23  24 math is divided...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>which fish would survive in salt waterat</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  what is the step by step guide to invest in sh...   \n",
       "1   1     3     4  what is the story of kohinoor koh  i  noor dia...   \n",
       "2   2     5     6  how can i increase the speed of my internet co...   \n",
       "3   3     7     8  why am i mentally very lonelyat how can i solv...   \n",
       "4   4     9    10  which one dissolve in water quikly sugar salt ...   \n",
       "\n",
       "                                           question2  is_duplicate  q1_len  \\\n",
       "0  what is the step by step guide to invest in sh...             0      67   \n",
       "1  what would happen if the indian government sto...             0      52   \n",
       "2  how can internet speed be increased by hacking...             0      74   \n",
       "3  find the remainder when 23  24 math is divided...             0      52   \n",
       "4           which fish would survive in salt waterat             0      75   \n",
       "\n",
       "   q2_len  q1_new_words  q2_new_words  \n",
       "0      58            14            12  \n",
       "1      89            12            17  \n",
       "2      60            14            10  \n",
       "3      57            11            13  \n",
       "4      40            13             7  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['q1_new_words']=df1['question1'].apply(lambda row: len(row.split(' ')))\n",
    "df1['q2_new_words']=df1['question2'].apply(lambda row: len(row.split(' ')))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d05cc5be-bec6-4c6d-97fc-53792b72145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# Store processed questions (without stopwords and stemmed)\n",
    "stemmer = PorterStemmer()\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = [stemmer.stem(word) for word in text.split() if word not in STOP_WORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df1['question1'] = df1['question1'].apply(preprocess_text)\n",
    "df1['question2'] = df1['question2'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a818cf3-bbb4-40b3-ba6a-cd4c9cf147fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reset gmail password rememb recoveri informationat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question2'][22122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c86cfcb0-a46e-4137-93ff-a9794c2050e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'process get surgic resid uk complet mbb indiaat'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['question2'][100000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "812793a0-e6f9-4bae-8463-066953d0eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words(row):\n",
    "    w1 = set(map(lambda word:word.lower().strip(),row['question1'].split(' ')))\n",
    "    w2 = set(map(lambda word:word.lower().strip(),row['question2'].split(' ')))\n",
    "    return len(w1 & w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab679e6c-58df-408d-9d2d-87a6b0ea0e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_new_words</th>\n",
       "      <th>q2_new_words</th>\n",
       "      <th>word_common</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>step step guid invest share market indiaat</td>\n",
       "      <td>step step guid invest share marketat</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>stori kohinoor koh noor diamondat</td>\n",
       "      <td>would happen indian govern stole kohinoor koh ...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>increas speed internet connect use vpnat</td>\n",
       "      <td>internet speed increas hack dnsat</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>mental lonelyat solv itat</td>\n",
       "      <td>find remaind 23 24 math divid 24 23at</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>one dissolv water quikli sugar salt methan car...</td>\n",
       "      <td>fish would surviv salt waterat</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2         step step guid invest share market indiaat   \n",
       "1   1     3     4                  stori kohinoor koh noor diamondat   \n",
       "2   2     5     6           increas speed internet connect use vpnat   \n",
       "3   3     7     8                          mental lonelyat solv itat   \n",
       "4   4     9    10  one dissolv water quikli sugar salt methan car...   \n",
       "\n",
       "                                           question2  is_duplicate  q1_len  \\\n",
       "0               step step guid invest share marketat             0      67   \n",
       "1  would happen indian govern stole kohinoor koh ...             0      52   \n",
       "2                  internet speed increas hack dnsat             0      74   \n",
       "3              find remaind 23 24 math divid 24 23at             0      52   \n",
       "4                     fish would surviv salt waterat             0      75   \n",
       "\n",
       "   q2_len  q1_new_words  q2_new_words  word_common  \n",
       "0      58            14            12            4  \n",
       "1      89            12            17            3  \n",
       "2      60            14            10            3  \n",
       "3      57            11            13            0  \n",
       "4      40            13             7            1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['word_common']=df1.apply(common_words,axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d91524c-5a84-4a45-9832-269365b9a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(row):\n",
    "    w1 = set(map(lambda word:word.lower().strip(),row['question1'].split(' ')))\n",
    "    w2 = set(map(lambda word:word.lower().strip(),row['question2'].split(' ')))\n",
    "    return (len(w1) + len(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef37f355-dd2b-4b8b-a49b-b280fd98922f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_new_words</th>\n",
       "      <th>q2_new_words</th>\n",
       "      <th>word_common</th>\n",
       "      <th>total_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>step step guid invest share market indiaat</td>\n",
       "      <td>step step guid invest share marketat</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>stori kohinoor koh noor diamondat</td>\n",
       "      <td>would happen indian govern stole kohinoor koh ...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>increas speed internet connect use vpnat</td>\n",
       "      <td>internet speed increas hack dnsat</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>mental lonelyat solv itat</td>\n",
       "      <td>find remaind 23 24 math divid 24 23at</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>one dissolv water quikli sugar salt methan car...</td>\n",
       "      <td>fish would surviv salt waterat</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2         step step guid invest share market indiaat   \n",
       "1   1     3     4                  stori kohinoor koh noor diamondat   \n",
       "2   2     5     6           increas speed internet connect use vpnat   \n",
       "3   3     7     8                          mental lonelyat solv itat   \n",
       "4   4     9    10  one dissolv water quikli sugar salt methan car...   \n",
       "\n",
       "                                           question2  is_duplicate  q1_len  \\\n",
       "0               step step guid invest share marketat             0      67   \n",
       "1  would happen indian govern stole kohinoor koh ...             0      52   \n",
       "2                  internet speed increas hack dnsat             0      74   \n",
       "3              find remaind 23 24 math divid 24 23at             0      52   \n",
       "4                     fish would surviv salt waterat             0      75   \n",
       "\n",
       "   q2_len  q1_new_words  q2_new_words  word_common  total_word  \n",
       "0      58            14            12            4          11  \n",
       "1      89            12            17            3          15  \n",
       "2      60            14            10            3          11  \n",
       "3      57            11            13            0          11  \n",
       "4      40            13             7            1          15  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['total_word']=df1.apply(total_words,axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4b1da58-1878-4b24-a35b-4135c9faa7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_new_words</th>\n",
       "      <th>q2_new_words</th>\n",
       "      <th>word_common</th>\n",
       "      <th>total_word</th>\n",
       "      <th>word_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>step step guid invest share market indiaat</td>\n",
       "      <td>step step guid invest share marketat</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>stori kohinoor koh noor diamondat</td>\n",
       "      <td>would happen indian govern stole kohinoor koh ...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>increas speed internet connect use vpnat</td>\n",
       "      <td>internet speed increas hack dnsat</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>mental lonelyat solv itat</td>\n",
       "      <td>find remaind 23 24 math divid 24 23at</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>one dissolv water quikli sugar salt methan car...</td>\n",
       "      <td>fish would surviv salt waterat</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2         step step guid invest share market indiaat   \n",
       "1   1     3     4                  stori kohinoor koh noor diamondat   \n",
       "2   2     5     6           increas speed internet connect use vpnat   \n",
       "3   3     7     8                          mental lonelyat solv itat   \n",
       "4   4     9    10  one dissolv water quikli sugar salt methan car...   \n",
       "\n",
       "                                           question2  is_duplicate  q1_len  \\\n",
       "0               step step guid invest share marketat             0      67   \n",
       "1  would happen indian govern stole kohinoor koh ...             0      52   \n",
       "2                  internet speed increas hack dnsat             0      74   \n",
       "3              find remaind 23 24 math divid 24 23at             0      52   \n",
       "4                     fish would surviv salt waterat             0      75   \n",
       "\n",
       "   q2_len  q1_new_words  q2_new_words  word_common  total_word  word_share  \n",
       "0      58            14            12            4          11        0.36  \n",
       "1      89            12            17            3          15        0.20  \n",
       "2      60            14            10            3          11        0.27  \n",
       "3      57            11            13            0          11        0.00  \n",
       "4      40            13             7            1          15        0.07  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['word_share']=round(df1['word_common']/df1['total_word'],2)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39ede0f1-38e3-4dfb-a575-3660d35a8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def fetch_token_features_with_stemming(row):\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    SAFE_DIV = 0.0001\n",
    "\n",
    "    STOP_WORDS = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    token_features = [0.0] * 8\n",
    "\n",
    "    # Tokenize and stem\n",
    "    q1_tokens = [stemmer.stem(word) for word in q1.split()]\n",
    "    q2_tokens = [stemmer.stem(word) for word in q2.split()]\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "\n",
    "    # Get the stopwords in questions\n",
    "    q1_stop = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stop = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "\n",
    "    # Get the common non-stopword from question pairs\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "\n",
    "    # Get the common stopword from question pairs\n",
    "    common_stop_count = len(q1_stop.intersection(q2_stop))\n",
    "\n",
    "    # Get the common token from question pairs\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "    # Compute token features\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stop), len(q2_stop)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stop), len(q2_stop)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "\n",
    "    # Last word of both questions is the same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "\n",
    "    # First word of both questions is the same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "\n",
    "    return token_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6589bc8b-7e6e-4276-8b39-98a8f97b7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features = df1.apply(fetch_token_features_with_stemming,axis = 1)\n",
    "\n",
    "df1['cwc_min']  = list(map(lambda x:x[0],token_features))\n",
    "df1['cwc_max']  = list(map(lambda x:x[1],token_features))\n",
    "df1['csc_min']  = list(map(lambda x:x[2],token_features))\n",
    "df1['csc_max']  = list(map(lambda x:x[3],token_features))\n",
    "df1['ctc_min']  = list(map(lambda x:x[4],token_features))\n",
    "df1['ctc_max']  = list(map(lambda x:x[5],token_features))\n",
    "df1['last_word_eq']  = list(map(lambda x:x[6],token_features))\n",
    "df1['first_word_eq']  = list(map(lambda x:x[7],token_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eaf113d9-eb78-4711-8ee1-2a884dcd48c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_new_words</th>\n",
       "      <th>q2_new_words</th>\n",
       "      <th>...</th>\n",
       "      <th>total_word</th>\n",
       "      <th>word_share</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>step step guid invest share market indiaat</td>\n",
       "      <td>step step guid invest share marketat</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.666656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666656</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>stori kohinoor koh noor diamondat</td>\n",
       "      <td>would happen indian govern stole kohinoor koh ...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.299997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.299997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>increas speed internet connect use vpnat</td>\n",
       "      <td>internet speed increas hack dnsat</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.499992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.499992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>mental lonelyat solv itat</td>\n",
       "      <td>find remaind 23 24 math divid 24 23at</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>one dissolv water quikli sugar salt methan car...</td>\n",
       "      <td>fish would surviv salt waterat</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.199996</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199996</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2         step step guid invest share market indiaat   \n",
       "1   1     3     4                  stori kohinoor koh noor diamondat   \n",
       "2   2     5     6           increas speed internet connect use vpnat   \n",
       "3   3     7     8                          mental lonelyat solv itat   \n",
       "4   4     9    10  one dissolv water quikli sugar salt methan car...   \n",
       "\n",
       "                                           question2  is_duplicate  q1_len  \\\n",
       "0               step step guid invest share marketat             0      67   \n",
       "1  would happen indian govern stole kohinoor koh ...             0      52   \n",
       "2                  internet speed increas hack dnsat             0      74   \n",
       "3              find remaind 23 24 math divid 24 23at             0      52   \n",
       "4                     fish would surviv salt waterat             0      75   \n",
       "\n",
       "   q2_len  q1_new_words  q2_new_words  ...  total_word  word_share   cwc_min  \\\n",
       "0      58            14            12  ...          11        0.36  0.799984   \n",
       "1      89            12            17  ...          15        0.20  0.599988   \n",
       "2      60            14            10  ...          11        0.27  0.599988   \n",
       "3      57            11            13  ...          11        0.00  0.000000   \n",
       "4      40            13             7  ...          15        0.07  0.199996   \n",
       "\n",
       "    cwc_max  csc_min  csc_max   ctc_min   ctc_max  last_word_eq  first_word_eq  \n",
       "0  0.666656      0.0      0.0  0.666656  0.571420           0.0            1.0  \n",
       "1  0.299997      0.0      0.0  0.599988  0.299997           0.0            0.0  \n",
       "2  0.499992      0.0      0.0  0.599988  0.499992           0.0            0.0  \n",
       "3  0.000000      0.0      0.0  0.000000  0.000000           0.0            0.0  \n",
       "4  0.099999      0.0      0.0  0.199996  0.099999           0.0            0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bdd78dd-107d-418e-8cee-dc873198384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "def fetch_length_features(row):\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    length_features=[0.0]*8\n",
    "    #converting the sentence into tokens\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return length_features\n",
    "\n",
    "\n",
    "    #absoloute length features\n",
    "    length_features[0]=abs(len(q1_tokens) - len(q2_tokens))\n",
    "\n",
    "    #avg token length of both question\n",
    "    length_features[1]=(len(q1_tokens) + len(q2_tokens))/2\n",
    "\n",
    "    strs = list(distance.lcsubstrings(q1,q2))\n",
    "    if  strs:\n",
    "        length_features[2] = len(strs[0])/(min(len(q1),len(q2))+1)\n",
    "    else:\n",
    "        length_features[2] = 0\n",
    "    return length_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c90dba7-fae3-4e8e-8b54-381dc7b945b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_features = df1.apply(fetch_length_features,axis = 1)\n",
    "df1['abs_len_diff']=list(map(lambda x:x[0],length_features))\n",
    "df1['mean_len']=list(map(lambda x:x[1],length_features))\n",
    "df1['longest_substr_ratio']=list(map(lambda x:x[2],length_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c2824db-9d6d-4d0f-b2f7-86695ef1328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuzzy features\n",
    "from fuzzywuzzy import fuzz\n",
    "def fetch_fuzzy_features(row):\n",
    "    q1 = row.get('question1','')\n",
    "    q2 = row.get('question2','')\n",
    "\n",
    "    fuzzy_features = [0.0]*4\n",
    "    #fuzzy ratio\n",
    "    fuzzy_features[0] = fuzz.ratio(q1,q2)\n",
    "\n",
    "     #fuzzy partial ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1,q2)\n",
    "\n",
    "     #fuzzy token sort ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1,q2)\n",
    "\n",
    "     #fuzzy token set ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1,q2)\n",
    "\n",
    "    return fuzzy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f0716c8-3a07-4ee4-ac42-2536b317e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_features = df1.apply(fetch_fuzzy_features, axis =1)\n",
    "#creating  new fuzzy features column for fuzzy features\n",
    "df1['fuzzy_ratio'] = list(map(lambda x : x[0],fuzzy_features))\n",
    "df1['fuzzy_partial_ratio'] = list(map(lambda x : x[1],fuzzy_features))\n",
    "df1['token_sort_ratio'] = list(map(lambda x : x[2],fuzzy_features))\n",
    "df1['token_set_ratio'] = list(map(lambda x : x[3],fuzzy_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "626c2f26-d614-4cdd-bf37-90282788cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Combine all questions for tokenization\n",
    "all_questions = df1['question1'].tolist() + df1['question2'].tolist()\n",
    "\n",
    "# Tokenize and convert to sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(all_questions)\n",
    "\n",
    "df1['q1_seq']= tokenizer.texts_to_sequences(df1['question1'])\n",
    "df1['q2_seq'] = tokenizer.texts_to_sequences(df1['question2'])\n",
    "\n",
    "# Pad sequences to equal length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ea9205b-9f69-49c1-9214-16b06f4a9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "df1['q1_padded'] = pad_sequences(df1['q1_seq'], maxlen=max_len, padding='post').tolist()\n",
    "df1['q2_padded'] = pad_sequences(df1['q2_seq'], maxlen=max_len, padding='post').tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d977af0-cdd2-4a8f-9b68-9640066bb612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [577, 577, 2213, 198, 422, 182, 12, 0, 0, 0, 0...\n",
      "1    [481, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "2    [117, 404, 492, 472, 4, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "3    [991, 4388, 426, 57, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "4    [11, 204, 2354, 2036, 1547, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: q1_padded, dtype: object\n",
      "0    [577, 577, 2213, 198, 422, 841, 0, 0, 0, 0, 0,...\n",
      "1    [8, 38, 19, 166, 3576, 816, 0, 0, 0, 0, 0, 0, ...\n",
      "2    [492, 404, 117, 211, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
      "3    [31, 3583, 2541, 1390, 335, 1661, 1390, 0, 0, ...\n",
      "4    [1832, 8, 1074, 2036, 780, 0, 0, 0, 0, 0, 0, 0...\n",
      "Name: q2_padded, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df1['q1_padded'].head())\n",
    "print(df1['q2_padded'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "caa366a1-1b57-437b-a93a-7b80809d1a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3796, 138, 1153, 949, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['q1_padded'][4445]\n",
    "df1['q2_padded'][4445]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5487b98-f0cc-4097-a6b6-20620f599363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Feature Matrix Shape: (200000, 22)\n",
      "First Row (Scaled): [0.10771704 0.0490279  0.1031746  0.04435484 0.17391304 0.08823529\n",
      " 0.72       0.01075269 0.10743802 0.92455646 0.92       0.94\n",
      " 0.87       0.85       0.79998748 0.66665845 0.         0.\n",
      " 0.66665972 0.57142398 0.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Extract the feature columns into a NumPy array\n",
    "features = [\n",
    "    \"q1_len\", \"q2_len\", \"q1_new_words\", \"q2_new_words\", \n",
    "    \"word_common\", \"total_word\", \"word_share\", \n",
    "    \"abs_len_diff\", \"mean_len\", \"longest_substr_ratio\", \n",
    "    \"fuzzy_ratio\", \"fuzzy_partial_ratio\", \"token_sort_ratio\",\n",
    "    \"token_set_ratio\", \"cwc_min\", \"cwc_max\", \"csc_min\", \"csc_max\",\n",
    "    \"ctc_min\", \"ctc_max\", \"last_word_eq\", \"first_word_eq\"\n",
    "]\n",
    "\n",
    "# Convert features to NumPy array\n",
    "X_features_raw = df1[features].values  # Shape: (num_samples, num_features)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the features\n",
    "X_features = scaler.fit_transform(X_features_raw)\n",
    "\n",
    "# Confirm the shape and scaled range of the features\n",
    "print(f\"Scaled Feature Matrix Shape: {X_features.shape}\")\n",
    "print(f\"First Row (Scaled): {X_features[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87cd3d25-68c8-4147-ab4a-070f6f6febd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Combine padded sequences and features\n",
    "###X_tokens = np.array(df['q1_padded'].tolist()) + np.array(df['q2_padded'].tolist())\n",
    "##X_tokens = np.hstack([np.array(df1['q1_padded'].tolist()), np.array(df1['q2_padded'].tolist())])\n",
    "X_q1 = np.array(df1['q1_padded'].tolist())  # Shape: (num_samples, max_len)\n",
    "X_q2 = np.array(df1['q2_padded'].tolist())  \n",
    "\n",
    "\n",
    "# Labels (use dummy labels for now, replace with actual labels)\n",
    "y =df1['is_duplicate'].values  # Example: 1 for duplicate, 0 for non-duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "530a57a5-4c95-4a00-96fe-ae02be56b476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n",
       "\n",
       " Q1_Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " Q2_Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span>  Q1_Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span>  Q2_Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " Features_Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "\n",
       " lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span>  Features_Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       "\n",
       " lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "\n",
       " lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "\n",
       " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               \n",
       "\n",
       " batch_normalization            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                                  \n",
       "\n",
       " concatenate_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             \n",
       "                                                                            dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           \n",
       "                                                                            batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
       "\n",
       " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,544</span>  concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "\n",
       " batch_normalization_1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                                  \n",
       "\n",
       " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>  batch_normalization_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " Q1_Input (\u001b[38;5;33mInputLayer\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " Q2_Input (\u001b[38;5;33mInputLayer\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m640,000\u001b[0m  Q1_Input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m640,000\u001b[0m  Q2_Input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " Features_Input (\u001b[38;5;33mInputLayer\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m131,584\u001b[0m  embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "\n",
       " lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m131,584\u001b[0m  embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m1,472\u001b[0m  Features_Input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       "\n",
       " lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                        \u001b[38;5;34m131,584\u001b[0m  lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "\n",
       " lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                        \u001b[38;5;34m131,584\u001b[0m  lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "\n",
       " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               \n",
       "\n",
       " batch_normalization            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                             \u001b[38;5;34m256\u001b[0m  dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                                  \n",
       "\n",
       " concatenate_1 (\u001b[38;5;33mConcatenate\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                              \u001b[38;5;34m0\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             \n",
       "                                                                            dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           \n",
       "                                                                            batch_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
       "\n",
       " dense_2 (\u001b[38;5;33mDense\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m20,544\u001b[0m  concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " dropout_3 (\u001b[38;5;33mDropout\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "\n",
       " batch_normalization_1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                             \u001b[38;5;34m256\u001b[0m  dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                                  \n",
       "\n",
       " dense_3 (\u001b[38;5;33mDense\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                               \u001b[38;5;34m65\u001b[0m  batch_normalization_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,828,929</span> (6.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,828,929\u001b[0m (6.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,828,673</span> (6.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,828,673\u001b[0m (6.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000\n",
    "embedding_dim = 128\n",
    "max_len = 20\n",
    "lstm_units = 128\n",
    "dropout_rate = 0.3\n",
    "l2_lambda = 0.001\n",
    "\n",
    "# Input for Question 1\n",
    "q1_input = Input(shape=(max_len,), name=\"Q1_Input\")\n",
    "q1_embedding = Embedding(vocab_size, embedding_dim, input_length=max_len)(q1_input)\n",
    "q1_lstm = LSTM(lstm_units, kernel_regularizer=l2(l2_lambda), return_sequences=True)(q1_embedding)\n",
    "q1_lstm = LSTM(lstm_units, kernel_regularizer=l2(l2_lambda))(q1_lstm)\n",
    "q1_lstm = Dropout(dropout_rate)(q1_lstm)\n",
    "\n",
    "# Input for Question 2\n",
    "q2_input = Input(shape=(max_len,), name=\"Q2_Input\")\n",
    "q2_embedding = Embedding(vocab_size, embedding_dim, input_length=max_len)(q2_input)\n",
    "q2_lstm = LSTM(lstm_units, kernel_regularizer=l2(l2_lambda), return_sequences=True)(q2_embedding)\n",
    "q2_lstm = LSTM(lstm_units, kernel_regularizer=l2(l2_lambda))(q2_lstm)\n",
    "q2_lstm = Dropout(dropout_rate)(q2_lstm)\n",
    "\n",
    "# Input for Additional Features\n",
    "features_input = Input(shape=(X_features.shape[1],), name=\"Features_Input\")\n",
    "features_dense = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_lambda))(features_input)\n",
    "features_dense = Dropout(dropout_rate)(features_dense)\n",
    "features_dense = BatchNormalization()(features_dense)\n",
    "\n",
    "# Combine all inputs\n",
    "combined = Concatenate()([q1_lstm, q2_lstm, features_dense])\n",
    "combined = Dense(64, activation=\"relu\", kernel_regularizer=l2(l2_lambda))(combined)\n",
    "combined = Dropout(dropout_rate)(combined)\n",
    "combined = BatchNormalization()(combined)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation=\"sigmoid\")(combined)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[q1_input, q2_input, features_input], outputs=output)\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4cc6fc77-e136-4a20-88da-f43ca4ca5aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m979s\u001b[0m 380ms/step - accuracy: 0.7218 - loss: 0.6972 - val_accuracy: 0.7901 - val_loss: 0.4502 - learning_rate: 5.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1420s\u001b[0m 568ms/step - accuracy: 0.7890 - loss: 0.4423 - val_accuracy: 0.7829 - val_loss: 0.4362 - learning_rate: 5.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1663s\u001b[0m 665ms/step - accuracy: 0.7963 - loss: 0.4231 - val_accuracy: 0.7922 - val_loss: 0.4281 - learning_rate: 5.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1116s\u001b[0m 430ms/step - accuracy: 0.7993 - loss: 0.4173 - val_accuracy: 0.7940 - val_loss: 0.4218 - learning_rate: 5.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m941s\u001b[0m 377ms/step - accuracy: 0.8012 - loss: 0.4128 - val_accuracy: 0.7951 - val_loss: 0.4222 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m923s\u001b[0m 369ms/step - accuracy: 0.8025 - loss: 0.4096 - val_accuracy: 0.7953 - val_loss: 0.4184 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 378ms/step - accuracy: 0.8065 - loss: 0.4007 - val_accuracy: 0.7953 - val_loss: 0.4166 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1323s\u001b[0m 529ms/step - accuracy: 0.8093 - loss: 0.3966 - val_accuracy: 0.7987 - val_loss: 0.4141 - learning_rate: 5.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m998s\u001b[0m 399ms/step - accuracy: 0.8140 - loss: 0.3889 - val_accuracy: 0.7843 - val_loss: 0.4221 - learning_rate: 5.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1038s\u001b[0m 415ms/step - accuracy: 0.8139 - loss: 0.3882 - val_accuracy: 0.7962 - val_loss: 0.4172 - learning_rate: 5.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1165s\u001b[0m 466ms/step - accuracy: 0.8175 - loss: 0.3800 - val_accuracy: 0.7945 - val_loss: 0.4234 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Callbacks to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "# Model training\n",
    "history = model.fit(\n",
    "    [X_q1, X_q2, X_features],\n",
    "    y,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5e14bff-daf5-49c0-850c-845b10155fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features: [ 6.          7.          2.          3.          4.          9.\n",
      "  0.44444444  1.          6.5         0.54285714 74.         77.\n",
      " 70.         74.          0.66666667  0.57142857  0.66666667  0.85714286\n",
      "  0.93333333  0.93333333  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text= text.replace('?',' ')\n",
    "    text= text.replace('.',' ')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features_v2(q1, q2):\n",
    "    # Clean questions\n",
    "    q1_clean = clean_text(q1)\n",
    "    q2_clean = clean_text(q2)\n",
    "    \n",
    "    # Split into words\n",
    "    q1_words = set(q1_clean.split())\n",
    "    q2_words = set(q2_clean.split())\n",
    "\n",
    "    # Length-based features\n",
    "    q1_len = len(q1_clean.split())\n",
    "    q2_len = len(q2_clean.split())\n",
    "    abs_len_diff = abs(q1_len - q2_len)\n",
    "    mean_len = (q1_len + q2_len) / 2\n",
    "\n",
    "    # Unique word features\n",
    "    q1_new_words = len(q1_words - q2_words)\n",
    "    q2_new_words = len(q2_words - q1_words)\n",
    "\n",
    "    # Word overlap features\n",
    "    word_common = len(q1_words & q2_words)\n",
    "    total_word = len(q1_words | q2_words)\n",
    "    word_share = word_common / total_word if total_word > 0 else 0\n",
    "\n",
    "    # Fuzzy matching features\n",
    "    fuzzy_ratio = fuzz.ratio(q1_clean, q2_clean)\n",
    "    fuzzy_partial_ratio = fuzz.partial_ratio(q1_clean, q2_clean)\n",
    "    token_sort_ratio = fuzz.token_sort_ratio(q1_clean, q2_clean)\n",
    "    token_set_ratio = fuzz.token_set_ratio(q1_clean, q2_clean)\n",
    "\n",
    "    # Longest common substring ratio\n",
    "    def longest_substring_ratio(a, b):\n",
    "        m, n = len(a), len(b)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        longest = 0\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if a[i - 1] == b[j - 1]:\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                    longest = max(longest, dp[i][j])\n",
    "        return longest / max(m, n) if max(m, n) > 0 else 0\n",
    "\n",
    "    longest_substr_ratio = longest_substring_ratio(q1_clean, q2_clean)\n",
    "\n",
    "    # Positional features\n",
    "    last_word_eq = int(q1_clean.split()[-1] == q2_clean.split()[-1]) if q1_clean and q2_clean else 0\n",
    "    first_word_eq = int(q1_clean.split()[0] == q2_clean.split()[0]) if q1_clean and q2_clean else 0\n",
    "\n",
    "    # Create combined word count features\n",
    "    def count_word_matches(w1, w2):\n",
    "        w1_set = set(w1)\n",
    "        w2_set = set(w2)\n",
    "        return len(w1_set & w2_set) / min(len(w1_set), len(w2_set)) if min(len(w1_set), len(w2_set)) > 0 else 0\n",
    "\n",
    "    cwc_min = count_word_matches(q1_words, q2_words)\n",
    "    cwc_max = len(q1_words & q2_words) / max(len(q1_words), len(q2_words)) if max(len(q1_words), len(q2_words)) > 0 else 0\n",
    "\n",
    "    csc_min = count_word_matches(q1_clean.split(), q2_clean.split())\n",
    "    csc_max = len(q1_clean.split()) / max(len(q1_clean.split()), len(q2_clean.split())) if max(len(q1_clean.split()), len(q2_clean.split())) > 0 else 0\n",
    "\n",
    "    ctc_min = count_word_matches(set(q1_clean), set(q2_clean))\n",
    "    ctc_max = len(set(q1_clean) & set(q2_clean)) / max(len(set(q1_clean)), len(set(q2_clean))) if max(len(set(q1_clean)), len(set(q2_clean))) > 0 else 0\n",
    "\n",
    "    # Combine all features\n",
    "    features = [\n",
    "        q1_len, q2_len, q1_new_words, q2_new_words, \n",
    "        word_common, total_word, word_share, \n",
    "        abs_len_diff, mean_len, longest_substr_ratio, \n",
    "        fuzzy_ratio, fuzzy_partial_ratio, token_sort_ratio, token_set_ratio, \n",
    "        cwc_min, cwc_max, csc_min, csc_max,\n",
    "        ctc_min, ctc_max, last_word_eq, first_word_eq\n",
    "    ]\n",
    "    return np.array(features)\n",
    "\n",
    "# Example questions\n",
    "example_q0 = \"What is the capital of France?\"\n",
    "example_q1 = \"Which city is the capital of nepal?\"\n",
    "\n",
    "# Extract features\n",
    "example_features = extract_features_v2(example_q0, example_q1)\n",
    "print(f\"Extracted Features: {example_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "83a14ed3-7085-44e3-8460-65c2a11c793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "Prediction: 0.3132\n",
      "Non-Duplicate Questions\n"
     ]
    }
   ],
   "source": [
    "# Convert the input questions into sequences\n",
    "example_q1_seq = tokenizer.texts_to_sequences([example_q0])  # Convert text to sequences\n",
    "example_q2_seq = tokenizer.texts_to_sequences([example_q1])\n",
    "\n",
    "# Pad the sequences to match the model's expected input shape\n",
    "example_q1_padded = pad_sequences(example_q1_seq, maxlen=max_len, padding='post')\n",
    "example_q2_padded = pad_sequences(example_q2_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Reshape feature array to 2D (1 sample, multiple features)\n",
    "example_features_reshaped = np.array(example_features).reshape(1, -1)\n",
    "\n",
    "# Scale the features using the same scaler used during training\n",
    "example_features_scaled = scaler.transform(example_features_reshaped)\n",
    "\n",
    "# Ensure all inputs are NumPy arrays and correctly formatted\n",
    "inputs = [example_q1_padded, example_q2_padded, example_features_scaled]\n",
    "\n",
    "# Make a prediction using the model\n",
    "prediction = model.predict(inputs)\n",
    "\n",
    "# Display the prediction result\n",
    "print(f\"Prediction: {prediction[0][0]:.4f}\")\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\"Duplicate Questions\")\n",
    "else:\n",
    "    print(\"Non-Duplicate Questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c51cf530-a6ca-4e48-9cb3-02e133f4563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features: [  2.           3.           0.           1.           2.\n",
      "   3.           0.66666667   1.           2.5          0.73913043\n",
      "  85.         100.          85.         100.           1.\n",
      "   0.66666667   1.           0.66666667   1.           0.91666667\n",
      "   1.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text= text.replace('?',' ')\n",
    "    text= text.replace('.',' ')\n",
    "    text= text.replace('AI','Artificial Intelligence')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features_v2(q1, q2):\n",
    "    # Clean questions\n",
    "    q1_clean = clean_text(q1)\n",
    "    q2_clean = clean_text(q2)\n",
    "    \n",
    "    # Split into words\n",
    "    q1_words = set(q1_clean.split())\n",
    "    q2_words = set(q2_clean.split())\n",
    "\n",
    "    # Length-based features\n",
    "    q1_len = len(q1_clean.split())\n",
    "    q2_len = len(q2_clean.split())\n",
    "    abs_len_diff = abs(q1_len - q2_len)\n",
    "    mean_len = (q1_len + q2_len) / 2\n",
    "\n",
    "    # Unique word features\n",
    "    q1_new_words = len(q1_words - q2_words)\n",
    "    q2_new_words = len(q2_words - q1_words)\n",
    "\n",
    "    # Word overlap features\n",
    "    word_common = len(q1_words & q2_words)\n",
    "    total_word = len(q1_words | q2_words)\n",
    "    word_share = word_common / total_word if total_word > 0 else 0\n",
    "\n",
    "    # Fuzzy matching features\n",
    "    fuzzy_ratio = fuzz.ratio(q1_clean, q2_clean)\n",
    "    fuzzy_partial_ratio = fuzz.partial_ratio(q1_clean, q2_clean)\n",
    "    token_sort_ratio = fuzz.token_sort_ratio(q1_clean, q2_clean)\n",
    "    token_set_ratio = fuzz.token_set_ratio(q1_clean, q2_clean)\n",
    "\n",
    "    # Longest common substring ratio\n",
    "    def longest_substring_ratio(a, b):\n",
    "        m, n = len(a), len(b)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        longest = 0\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if a[i - 1] == b[j - 1]:\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                    longest = max(longest, dp[i][j])\n",
    "        return longest / max(m, n) if max(m, n) > 0 else 0\n",
    "\n",
    "    longest_substr_ratio = longest_substring_ratio(q1_clean, q2_clean)\n",
    "\n",
    "    # Positional features\n",
    "    last_word_eq = int(q1_clean.split()[-1] == q2_clean.split()[-1]) if q1_clean and q2_clean else 0\n",
    "    first_word_eq = int(q1_clean.split()[0] == q2_clean.split()[0]) if q1_clean and q2_clean else 0\n",
    "\n",
    "    # Create combined word count features\n",
    "    def count_word_matches(w1, w2):\n",
    "        w1_set = set(w1)\n",
    "        w2_set = set(w2)\n",
    "        return len(w1_set & w2_set) / min(len(w1_set), len(w2_set)) if min(len(w1_set), len(w2_set)) > 0 else 0\n",
    "\n",
    "    cwc_min = count_word_matches(q1_words, q2_words)\n",
    "    cwc_max = len(q1_words & q2_words) / max(len(q1_words), len(q2_words)) if max(len(q1_words), len(q2_words)) > 0 else 0\n",
    "\n",
    "    csc_min = count_word_matches(q1_clean.split(), q2_clean.split())\n",
    "    csc_max = len(q1_clean.split()) / max(len(q1_clean.split()), len(q2_clean.split())) if max(len(q1_clean.split()), len(q2_clean.split())) > 0 else 0\n",
    "\n",
    "    ctc_min = count_word_matches(set(q1_clean), set(q2_clean))\n",
    "    ctc_max = len(set(q1_clean) & set(q2_clean)) / max(len(set(q1_clean)), len(set(q2_clean))) if max(len(set(q1_clean)), len(set(q2_clean))) > 0 else 0\n",
    "\n",
    "    # Combine all features\n",
    "    features = [\n",
    "        q1_len, q2_len, q1_new_words, q2_new_words, \n",
    "        word_common, total_word, word_share, \n",
    "        abs_len_diff, mean_len, longest_substr_ratio, \n",
    "        fuzzy_ratio, fuzzy_partial_ratio, token_sort_ratio, token_set_ratio, \n",
    "        cwc_min, cwc_max, csc_min, csc_max,\n",
    "        ctc_min, ctc_max, last_word_eq, first_word_eq\n",
    "    ]\n",
    "    return np.array(features)\n",
    "\n",
    "# Example questions\n",
    "example_q1 = \"What is Artificial Intelligence?\"\n",
    "example_q2 = \"define Artificial Intelligence.\"\n",
    "\n",
    "# Extract features\n",
    "example_features = extract_features_v2(example_q1, example_q2)\n",
    "print(f\"Extracted Features: {example_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a229f6a-985e-4772-9bd5-bff8d50e3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "Prediction: 0.6840\n",
      "Duplicate Questions\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming you used the same tokenizer during training\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts([example_q1, example_q2])  # Fit tokenizer on sample questions\n",
    "\n",
    "# Convert questions to sequences\n",
    "example_q1_seq = tokenizer.texts_to_sequences([example_q1])\n",
    "example_q2_seq = tokenizer.texts_to_sequences([example_q2])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "example_q1_padded = pad_sequences(example_q1_seq, maxlen=max_len, padding='post')\n",
    "example_q2_padded = pad_sequences(example_q2_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Reshape feature array to 2D (1 sample, multiple features)\n",
    "example_features_reshaped = example_features.reshape(1, -1)\n",
    "\n",
    "# Scale the features using the same scaler used during training\n",
    "example_features_scaled = scaler.transform(example_features_reshaped)\n",
    "\n",
    "# Predict using the trained model\n",
    "prediction = model.predict(\n",
    "    [example_q1_padded, example_q2_padded, example_features_scaled]\n",
    ")\n",
    "\n",
    "# Display the prediction\n",
    "print(f\"Prediction: {prediction[0][0]:.4f}\")\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\"Duplicate Questions\")\n",
    "else:\n",
    "    print(\"Non-Duplicate Questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6fc895e6-48d7-40d2-94f3-0f87e845db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features: [ 6.          7.          2.          3.          4.          9.\n",
      "  0.44444444  1.          6.5         0.54285714 74.         77.\n",
      " 70.         74.          0.66666667  0.57142857  0.66666667  0.85714286\n",
      "  0.93333333  0.93333333  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text= text.replace('?',' ')\n",
    "    text= text.replace('.',' ')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features_v2(q1, q2):\n",
    "    # Clean questions\n",
    "    q1_clean = clean_text(q1)\n",
    "    q2_clean = clean_text(q2)\n",
    "    \n",
    "    # Split into words\n",
    "    q1_words = set(q1_clean.split())\n",
    "    q2_words = set(q2_clean.split())\n",
    "\n",
    "    # Length-based features\n",
    "    q1_len = len(q1_clean.split())\n",
    "    q2_len = len(q2_clean.split())\n",
    "    abs_len_diff = abs(q1_len - q2_len)\n",
    "    mean_len = (q1_len + q2_len) / 2\n",
    "\n",
    "    # Unique word features\n",
    "    q1_new_words = len(q1_words - q2_words)\n",
    "    q2_new_words = len(q2_words - q1_words)\n",
    "\n",
    "    # Word overlap features\n",
    "    word_common = len(q1_words & q2_words)\n",
    "    total_word = len(q1_words | q2_words)\n",
    "    word_share = word_common / total_word if total_word > 0 else 0\n",
    "\n",
    "    # Fuzzy matching features\n",
    "    fuzzy_ratio = fuzz.ratio(q1_clean, q2_clean)\n",
    "    fuzzy_partial_ratio = fuzz.partial_ratio(q1_clean, q2_clean)\n",
    "    token_sort_ratio = fuzz.token_sort_ratio(q1_clean, q2_clean)\n",
    "    token_set_ratio = fuzz.token_set_ratio(q1_clean, q2_clean)\n",
    "\n",
    "    # Longest common substring ratio\n",
    "    def longest_substring_ratio(a, b):\n",
    "        m, n = len(a), len(b)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        longest = 0\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if a[i - 1] == b[j - 1]:\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                    longest = max(longest, dp[i][j])\n",
    "        return longest / max(m, n) if max(m, n) > 0 else 0\n",
    "\n",
    "    longest_substr_ratio = longest_substring_ratio(q1_clean, q2_clean)\n",
    "\n",
    "    # Positional features\n",
    "    last_word_eq = int(q1_clean.split()[-1] == q2_clean.split()[-1]) if q1_clean and q2_clean else 0\n",
    "    first_word_eq = int(q1_clean.split()[0] == q2_clean.split()[0]) if q1_clean and q2_clean else 0\n",
    "\n",
    "    # Create combined word count features\n",
    "    def count_word_matches(w1, w2):\n",
    "        w1_set = set(w1)\n",
    "        w2_set = set(w2)\n",
    "        return len(w1_set & w2_set) / min(len(w1_set), len(w2_set)) if min(len(w1_set), len(w2_set)) > 0 else 0\n",
    "\n",
    "    cwc_min = count_word_matches(q1_words, q2_words)\n",
    "    cwc_max = len(q1_words & q2_words) / max(len(q1_words), len(q2_words)) if max(len(q1_words), len(q2_words)) > 0 else 0\n",
    "\n",
    "    csc_min = count_word_matches(q1_clean.split(), q2_clean.split())\n",
    "    csc_max = len(q1_clean.split()) / max(len(q1_clean.split()), len(q2_clean.split())) if max(len(q1_clean.split()), len(q2_clean.split())) > 0 else 0\n",
    "\n",
    "    ctc_min = count_word_matches(set(q1_clean), set(q2_clean))\n",
    "    ctc_max = len(set(q1_clean) & set(q2_clean)) / max(len(set(q1_clean)), len(set(q2_clean))) if max(len(set(q1_clean)), len(set(q2_clean))) > 0 else 0\n",
    "\n",
    "    # Combine all features\n",
    "    features = [\n",
    "        q1_len, q2_len, q1_new_words, q2_new_words, \n",
    "        word_common, total_word, word_share, \n",
    "        abs_len_diff, mean_len, longest_substr_ratio, \n",
    "        fuzzy_ratio, fuzzy_partial_ratio, token_sort_ratio, token_set_ratio, \n",
    "        cwc_min, cwc_max, csc_min, csc_max,\n",
    "        ctc_min, ctc_max, last_word_eq, first_word_eq\n",
    "    ]\n",
    "    return np.array(features)\n",
    "\n",
    "# Example questions\n",
    "example_q = \"What is the capital of france?\"\n",
    "example_q1 = \"Which city is the capital of Nepal?\"\n",
    "\n",
    "# Extract features\n",
    "example_features = extract_features_v2(example_q, example_q1)\n",
    "print(f\"Extracted Features: {example_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ee0e6f7f-e8a7-45e7-a375-c6edad15930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
      "Prediction: 0.4641\n",
      "Non-Duplicate Questions\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming you used the same tokenizer during training\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts([example_q, example_q1])  # Fit tokenizer on sample questions\n",
    "\n",
    "# Convert questions to sequences\n",
    "example_q1_seq = tokenizer.texts_to_sequences([example_q])\n",
    "example_q2_seq = tokenizer.texts_to_sequences([example_q1])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "example_q1_padded = pad_sequences(example_q1_seq, maxlen=max_len, padding='post')\n",
    "example_q2_padded = pad_sequences(example_q2_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Reshape feature array to 2D (1 sample, multiple features)\n",
    "example_features_reshaped = example_features.reshape(1, -1)\n",
    "\n",
    "# Scale the features using the same scaler used during training\n",
    "example_features_scaled = scaler.transform(example_features_reshaped)\n",
    "\n",
    "# Predict using the trained model\n",
    "prediction = model.predict(\n",
    "    [example_q1_padded, example_q2_padded, example_features_scaled]\n",
    ")\n",
    "\n",
    "# Display the prediction\n",
    "print(f\"Prediction: {prediction[0][0]:.4f}\")\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\"Duplicate Questions\")\n",
    "else:\n",
    "    print(\"Non-Duplicate Questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f840a262-08c3-4340-9201-1184f07df855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming `model` is your trained Keras model and `scaler` is your feature scaler\n",
    "with open('quora_duplicate_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0b6f531-73f5-48fc-8302-62434e83b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl','wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7eabb-c475-40c4-a48d-9d003b8454fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
